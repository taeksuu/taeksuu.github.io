<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Taeksoo Kim</title>
    <meta name="author" content="Taeksoo Kim">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="images/favicon/taeksoo.ico" type="image/x-icon">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico?v=20260205" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>
  <body>
    <div class="page">
      <header class="hero">
        <div class="hero-text">
          <h1 class="name">Taeksoo Kim</h1>
          <p class="intro">
            Hi, I'm a 3rd year MS/PhD student at Seoul National University, where I am advised by Prof.
            <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">Hanbyul Joo</a>.
            My current research interests lie in simulating the physical world with agents using video generative models.
            I received a B.S in Industrial Engineering and a B.S. in Computer Science and Engineering from Seoul National University.
          </p>
          <p class="now">
            I'm currently seeking research internship opportunities starting anytime. <br> Feel free to reach out if you're interested in collaboration!
          </p>
          <nav class="link-row" aria-label="Primary">
            <a href="mailto:taeksu98@snu.ac.kr" target="_blank" rel="noopener">Email</a>
            <a href="data/CV_260205.pdf" target="_blank" rel="noopener">CV</a>
            <a href="https://scholar.google.com/citations?user=XE7pErYAAAAJ&hl=en" target="_blank" rel="noopener" class="icon-link" aria-label="Google Scholar">
              <svg class="icon" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
                <path d="M12 3L1 9l11 6 9-4.91V17h2V9L12 3zm0 11.27L5.74 11 12 7.73 18.26 11 12 14.27z"></path>
                <path d="M5 13.5V18l7 3 7-3v-4.5l-7 3-7-3z"></path>
              </svg>
              Scholar
            </a>
            <a href="https://github.com/taeksuu/" target="_blank" rel="noopener" class="icon-link" aria-label="GitHub">
              <svg class="icon" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
                <path d="M12 2C6.48 2 2 6.58 2 12.25c0 4.58 2.87 8.46 6.84 9.83.5.09.68-.22.68-.49 0-.24-.01-.88-.01-1.73-2.78.62-3.37-1.36-3.37-1.36-.45-1.17-1.11-1.48-1.11-1.48-.91-.64.07-.63.07-.63 1 .07 1.53 1.07 1.53 1.07.9 1.55 2.36 1.1 2.94.84.09-.66.35-1.1.63-1.35-2.22-.26-4.56-1.14-4.56-5.07 0-1.12.39-2.03 1.02-2.74-.1-.26-.44-1.31.1-2.73 0 0 .84-.27 2.75 1.05a9.3 9.3 0 0 1 2.5-.35c.85 0 1.71.12 2.5.35 1.91-1.32 2.75-1.05 2.75-1.05.54 1.42.2 2.47.1 2.73.63.71 1.02 1.62 1.02 2.74 0 3.94-2.34 4.81-4.57 5.07.36.32.69.94.69 1.9 0 1.37-.01 2.47-.01 2.81 0 .27.18.59.69.49A10.26 10.26 0 0 0 22 12.25C22 6.58 17.52 2 12 2z"></path>
              </svg>
              GitHub
            </a>
            <a href="https://www.linkedin.com/in/taeksoo-kim-245368351/" target="_blank" rel="noopener" class="icon-link" aria-label="LinkedIn">
              <svg class="icon icon-linkedin" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
                <path d="M4.98 3.5C4.98 4.88 3.86 6 2.48 6S0 4.88 0 3.5 1.12 1 2.5 1 4.98 2.12 4.98 3.5zM.5 8.5h4V24h-4V8.5zM8.5 8.5h3.83v2.12h.05c.53-1.01 1.83-2.08 3.77-2.08 4.03 0 4.77 2.66 4.77 6.12V24h-4v-7.52c0-1.79-.03-4.09-2.49-4.09-2.5 0-2.88 1.95-2.88 3.96V24h-4V8.5z"></path>
              </svg>
              LinkedIn
            </a>
          </nav>
        </div>
        <div class="hero-media">
          <a href="images/profile_main_resized.jpg" target="_blank" rel="noopener">
            <img
              src="images/profile_main_resized.jpg"
              alt="Portrait of Taeksoo Kim"
              loading="eager"
              decoding="async"
              fetchpriority="high"
            >
          </a>
        </div>
      </header>

      <section class="section">
        <h2>News</h2>
        <ul class="news-list">
          <li><strong>Jan 2026</strong>: Our work, <a href="https://taeksuu.github.io/tavid/" target="_blank" rel="noopener">Target-Aware Video Diffusion Models</a>, got accepted in ICLR 2026. See you in Rio, Brazil!</li>
          <li><strong>Mar 2023</strong>: Joined <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">SNU Visual Computing Lab</a>.</li>
        </ul>
      </section>

      <section class="section">
        <h2>Research</h2>

        <article class="publication">
          <div class="publication-media">
            <video muted autoplay loop playsinline preload="metadata" poster="images/dwm/image.png">
              <source src="images/dwm/video_resized.mp4" type="video/mp4">
              <img src="images/dwm/image.png" alt="Dexterous World Models preview" loading="lazy" decoding="async">
            </video>
          </div>
          <div class="publication-text">
            <a href="https://snuvclab.github.io/dwm/" class="paper-title" target="_blank" rel="noopener">Dexterous World Models</a>
            <p class="paper-authors">
              <a href="https://bjkim95.github.io/" target="_blank" rel="noopener">Byungjun Kim*</a>,
              <strong>Taeksoo Kim*</strong>,
              <a href="https://junc0ng.github.io/" target="_blank" rel="noopener">Junyoung Lee</a>,
              <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">Hanbyul Joo</a>
            </p>
            <p class="paper-meta"><span class="venue">arXiv 2025</span></p>
            <p class="paper-links">
              <a href="https://snuvclab.github.io/dwm/" target="_blank" rel="noopener">project page</a>
              <span aria-hidden="true">/</span>
              <a href="https://github.com/snuvclab/dwm" target="_blank" rel="noopener">code</a>
              <span aria-hidden="true">/</span>
              <a href="https://arxiv.org/abs/2512.17907" target="_blank" rel="noopener">arXiv</a>
            </p>
            <p class="paper-summary">
              We present DWM, a scene-action-conditioned video diffusion model that simulates dexterous human interactions in static 3D scenes.
            </p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-media">
            <video muted autoplay loop playsinline preload="metadata" poster="images/tavid/image.png">
              <source src="images/tavid/video.mp4" type="video/mp4">
              <img src="images/tavid/image.png" alt="Target-Aware Video Diffusion Models preview" loading="lazy" decoding="async">
            </video>
          </div>
          <div class="publication-text">
            <a href="https://taeksuu.github.io/tavid/" class="paper-title" target="_blank" rel="noopener">Target-Aware Video Diffusion Models</a>
            <p class="paper-authors">
              <strong>Taeksoo Kim</strong>,
              <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">Hanbyul Joo</a>
            </p>
            <p class="paper-meta"><span class="venue">ICLR 2026</span></p>
            <p class="paper-links">
              <a href="https://taeksuu.github.io/tavid/" target="_blank" rel="noopener">project page</a>
              <span aria-hidden="true">/</span>
              <a href="https://github.com/taeksuu/tavid" target="_blank" rel="noopener">code &amp; data</a>
              <span aria-hidden="true">/</span>
              <a href="https://arxiv.org/abs/2503.18950" target="_blank" rel="noopener">arXiv</a>
              <span aria-hidden="true">/</span>
              <a href="https://openreview.net/forum?id=311AxWM8FU&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2026%2FConference%2FAuthors%23your-submissions)" target="_blank" rel="noopener">openreview</a>
            </p>
            <p class="paper-summary">
              Our target-aware video diffusion model generates a video in which an actor accurately interacts with the target, specified with its segmentation mask.
            </p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-media">
            <video muted autoplay loop playsinline preload="metadata" poster="images/gala/image.png">
              <source src="images/gala/video_reversed.mp4" type="video/mp4">
              <img src="images/gala/image.png" alt="GALA preview" loading="lazy" decoding="async">
            </video>
          </div>
          <div class="publication-text">
            <a href="https://snuvclab.github.io/gala/" class="paper-title" target="_blank" rel="noopener">GALA: Generating Animatable Layered Assets from a Single Scan</a>
            <p class="paper-authors">
              <strong>Taeksoo Kim*</strong>,
              <a href="https://bjkim95.github.io/" target="_blank" rel="noopener">Byungjun Kim*</a>,
              <a href="https://shunsukesaito.github.io/" target="_blank" rel="noopener">Shunsuke Saito</a>,
              <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">Hanbyul Joo</a>
            </p>
            <p class="paper-meta"><span class="venue">CVPR 2024</span></p>
            <p class="paper-links">
              <a href="https://snuvclab.github.io/gala/" target="_blank" rel="noopener">project page</a>
              <span aria-hidden="true">/</span>
              <a href="https://github.com/snuvclab/GALA" target="_blank" rel="noopener">code</a>
              <span aria-hidden="true">/</span>
              <a href="https://arxiv.org/abs/2401.12979" target="_blank" rel="noopener">arXiv</a>
            </p>
            <p class="paper-summary">
              We present a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets.
            </p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-media">
            <video muted autoplay loop playsinline preload="metadata" poster="images/ncho/image.png">
              <source src="images/ncho/video.mp4" type="video/mp4">
              <img src="images/ncho/image.png" alt="NCHO preview" loading="lazy" decoding="async">
            </video>
          </div>
          <div class="publication-text">
            <a href="https://taeksuu.github.io/ncho/" class="paper-title" target="_blank" rel="noopener">NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects</a>
            <p class="paper-authors">
              <strong>Taeksoo Kim</strong>,
              <a href="https://shunsukesaito.github.io/" target="_blank" rel="noopener">Shunsuke Saito</a>,
              <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">Hanbyul Joo</a>
            </p>
            <p class="paper-meta"><span class="venue">ICCV 2023</span></p>
            <p class="paper-links">
              <a href="https://taeksuu.github.io/ncho" target="_blank" rel="noopener">project page</a>
              <span aria-hidden="true">/</span>
              <a href="https://github.com/taeksuu/ncho" target="_blank" rel="noopener">code &amp; data</a>
              <span aria-hidden="true">/</span>
              <a href="https://arxiv.org/abs/2305.14345" target="_blank" rel="noopener">arXiv</a>
            </p>
            <p class="paper-summary">
              We present a framework for learning a compositional generative model of humans and objects (backpacks, coats, and more) from real-world 3D scans.
            </p>
          </div>
        </article>
      </section>

      <footer class="footer">
        <p>Visual Computing Lab, Seoul National University</p>
        <p>Template adapted from <a href="https://github.com/jonbarron/jonbarron_website" target="_blank" rel="noopener">Jon Barron's website</a></p>
      </footer>
    </div>
  </body>
</html>
