<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Taeksoo Kim</title>
    <meta name="author" content="Taeksoo Kim">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>
  <body>
    <div class="page">
      <header class="hero">
        <div class="hero-text">
          <h1 class="name">Taeksoo Kim</h1>
          <p class="intro">
            Hi, I'm a 3rd year MS/PhD student at Seoul National University, where I am advised by Prof.
            <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">Hanbyul Joo</a>.
            My current research interests lie in simulating the physical world with agents using video generative models.
            I received a B.S in Industrial Engineering and a B.S. in Computer Science and Engineering from Seoul National University.
          </p>
          <p class="now">
            I'm currently seeking research internship opportunities starting anytime. <br> Feel free to reach out if you're interested in collaboration!
          </p>
          <nav class="link-row" aria-label="Primary">
            <a href="mailto:taeksu98@snu.ac.kr" target="_blank" rel="noopener">Email</a>
            <a href="data/CV_260205.pdf" target="_blank" rel="noopener">CV</a>
            <a href="https://scholar.google.com/citations?user=XE7pErYAAAAJ&hl=en" target="_blank" rel="noopener">Scholar</a>
            <a href="https://github.com/taeksuu/" target="_blank" rel="noopener">GitHub</a>
            <a href="https://www.linkedin.com/in/taeksoo-kim-245368351/" target="_blank" rel="noopener">LinkedIn</a>
          </nav>
        </div>
        <div class="hero-media">
          <a href="images/profile_main_resized.jpg" target="_blank" rel="noopener">
            <img
              src="images/profile_main_resized.jpg"
              alt="Portrait of Taeksoo Kim"
              loading="eager"
              decoding="async"
              fetchpriority="high"
            >
          </a>
        </div>
      </header>

      <section class="section">
        <h2>News</h2>
        <ul class="news-list">
          <li><strong>Jan 2026</strong>: Our work, <a href="https://taeksuu.github.io/tavid/" target="_blank" rel="noopener">Target-Aware Video Diffusion Models</a>, got accepted in ICLR 2026. See you in Rio, Brazil!</li>
          <li><strong>Mar 2023</strong>: Joined <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">SNU Visual Computing Lab</a>.</li>
        </ul>
      </section>

      <section class="section">
        <h2>Research</h2>

        <article class="publication">
          <div class="publication-media">
            <video muted autoplay loop playsinline preload="metadata" poster="images/dwm/image.png">
              <source src="images/dwm/video_resized.mp4" type="video/mp4">
              <img src="images/dwm/image.png" alt="Dexterous World Models preview" loading="lazy" decoding="async">
            </video>
          </div>
          <div class="publication-text">
            <a href="https://snuvclab.github.io/dwm/" class="paper-title" target="_blank" rel="noopener">Dexterous World Models</a>
            <p class="paper-authors">
              <a href="https://bjkim95.github.io/" target="_blank" rel="noopener">Byungjun Kim*</a>,
              <strong>Taeksoo Kim*</strong>,
              <a href="https://junc0ng.github.io/" target="_blank" rel="noopener">Junyoung Lee</a>,
              <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">Hanbyul Joo</a>
            </p>
            <p class="paper-meta"><span class="venue">arXiv 2025</span></p>
            <p class="paper-links">
              <a href="https://snuvclab.github.io/dwm/" target="_blank" rel="noopener">project page</a>
              <span aria-hidden="true">/</span>
              <a href="https://github.com/snuvclab/dwm" target="_blank" rel="noopener">code</a>
              <span aria-hidden="true">/</span>
              <a href="https://arxiv.org/abs/2512.17907" target="_blank" rel="noopener">arXiv</a>
            </p>
            <p class="paper-summary">
              We present DWM, a scene-action-conditioned video diffusion model that simulates dexterous human interactions in static 3D scenes.
            </p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-media">
            <video muted autoplay loop playsinline preload="metadata" poster="images/tavid/image.png">
              <source src="images/tavid/video.mp4" type="video/mp4">
              <img src="images/tavid/image.png" alt="Target-Aware Video Diffusion Models preview" loading="lazy" decoding="async">
            </video>
          </div>
          <div class="publication-text">
            <a href="https://taeksuu.github.io/tavid/" class="paper-title" target="_blank" rel="noopener">Target-Aware Video Diffusion Models</a>
            <p class="paper-authors">
              <strong>Taeksoo Kim</strong>,
              <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">Hanbyul Joo</a>
            </p>
            <p class="paper-meta"><span class="venue">ICLR 2026</span></p>
            <p class="paper-links">
              <a href="https://taeksuu.github.io/tavid/" target="_blank" rel="noopener">project page</a>
              <span aria-hidden="true">/</span>
              <a href="https://github.com/taeksuu/tavid" target="_blank" rel="noopener">code &amp; data</a>
              <span aria-hidden="true">/</span>
              <a href="https://arxiv.org/abs/2503.18950" target="_blank" rel="noopener">arXiv</a>
              <span aria-hidden="true">/</span>
              <a href="https://openreview.net/forum?id=311AxWM8FU&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2026%2FConference%2FAuthors%23your-submissions)" target="_blank" rel="noopener">openreview</a>
            </p>
            <p class="paper-summary">
              Given an input image, our target-aware video diffusion model generates a video in which an actor accurately interacts with the target, specified with its segmentation mask.
            </p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-media">
            <video muted autoplay loop playsinline preload="metadata" poster="images/gala/image.png">
              <source src="images/gala/video_reversed.mp4" type="video/mp4">
              <img src="images/gala/image.png" alt="GALA preview" loading="lazy" decoding="async">
            </video>
          </div>
          <div class="publication-text">
            <a href="https://snuvclab.github.io/gala/" class="paper-title" target="_blank" rel="noopener">GALA: Generating Animatable Layered Assets from a Single Scan</a>
            <p class="paper-authors">
              <strong>Taeksoo Kim*</strong>,
              <a href="https://bjkim95.github.io/" target="_blank" rel="noopener">Byungjun Kim*</a>,
              <a href="https://shunsukesaito.github.io/" target="_blank" rel="noopener">Shunsuke Saito</a>,
              <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">Hanbyul Joo</a>
            </p>
            <p class="paper-meta"><span class="venue">CVPR 2024</span></p>
            <p class="paper-links">
              <a href="https://snuvclab.github.io/gala/" target="_blank" rel="noopener">project page</a>
              <span aria-hidden="true">/</span>
              <a href="https://github.com/snuvclab/GALA" target="_blank" rel="noopener">code</a>
              <span aria-hidden="true">/</span>
              <a href="https://arxiv.org/abs/2401.12979" target="_blank" rel="noopener">arXiv</a>
            </p>
            <p class="paper-summary">
              We present a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets.
            </p>
          </div>
        </article>

        <article class="publication">
          <div class="publication-media">
            <video muted autoplay loop playsinline preload="metadata" poster="images/ncho/image.png">
              <source src="images/ncho/video.mp4" type="video/mp4">
              <img src="images/ncho/image.png" alt="NCHO preview" loading="lazy" decoding="async">
            </video>
          </div>
          <div class="publication-text">
            <a href="https://taeksuu.github.io/ncho/" class="paper-title" target="_blank" rel="noopener">NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects</a>
            <p class="paper-authors">
              <strong>Taeksoo Kim</strong>,
              <a href="https://shunsukesaito.github.io/" target="_blank" rel="noopener">Shunsuke Saito</a>,
              <a href="https://jhugestar.github.io/" target="_blank" rel="noopener">Hanbyul Joo</a>
            </p>
            <p class="paper-meta"><span class="venue">ICCV 2023</span></p>
            <p class="paper-links">
              <a href="https://taeksuu.github.io/ncho" target="_blank" rel="noopener">project page</a>
              <span aria-hidden="true">/</span>
              <a href="https://github.com/taeksuu/ncho" target="_blank" rel="noopener">code &amp; data</a>
              <span aria-hidden="true">/</span>
              <a href="https://arxiv.org/abs/2305.14345" target="_blank" rel="noopener">arXiv</a>
            </p>
            <p class="paper-summary">
              We present a novel framework for learning a compositional generative model of humans and objects (backpacks, coats, scarves, and more) from real-world 3D scans.
            </p>
          </div>
        </article>
      </section>

      <footer class="footer">
        <p>Visual Computing Lab, Seoul National University</p>
        <p>Template adapted from <a href="https://github.com/jonbarron/jonbarron_website" target="_blank" rel="noopener">Jon Barron's website</a></p>
      </footer>
    </div>
  </body>
</html>
